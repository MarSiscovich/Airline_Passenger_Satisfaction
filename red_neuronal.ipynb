{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: Desarrollo de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17317\n",
      "0.688878623397638\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[271], line 100\u001b[0m\n\u001b[0;32m     96\u001b[0m Z1, A1, Z2, A2, Z3, A3 \u001b[38;5;241m=\u001b[39m forward_prop(X_sample)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# distribuir error a través de la retropropagación\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# y devolver pendientes para pesos y sesgos\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m dW1, dB1, dW2, dB2, dW3, dB3 \u001b[38;5;241m=\u001b[39m \u001b[43mbackward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# actualizar pesos y sesgos\u001b[39;00m\n\u001b[0;32m    103\u001b[0m w_hidden \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m L \u001b[38;5;241m*\u001b[39m dW1\n",
      "Cell \u001b[1;32mIn[271], line 79\u001b[0m, in \u001b[0;36mbackward_prop\u001b[1;34m(Z1, A1, Z2, A2, Z3, A3, X, Y)\u001b[0m\n\u001b[0;32m     76\u001b[0m dC_dB3 \u001b[38;5;241m=\u001b[39m dC_dA3 \u001b[38;5;241m@\u001b[39m dA3_dZ3 \u001b[38;5;241m*\u001b[39m dZ3_dB3         \u001b[38;5;66;03m# (1x1) @ (1x1) * (1x1) = (1x1)   == (1x1)  B3\u001b[39;00m\n\u001b[0;32m     77\u001b[0m dC_dA2 \u001b[38;5;241m=\u001b[39m (dC_dA3 \u001b[38;5;241m@\u001b[39m dA3_dZ3 \u001b[38;5;241m@\u001b[39m dZ3_dA2)\u001b[38;5;241m.\u001b[39mT     \u001b[38;5;66;03m# (1x1) @ (1x1) @ (1x4) = (4x1)   == (4x1)  A2\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m dC_dW2 \u001b[38;5;241m=\u001b[39m \u001b[43mdC_dA2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdA2_dZ2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdZ2_dW2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m       \u001b[38;5;66;03m# (4x1) @ (1x4) @ (1x8) = (1x8)   == (4x8)  W2\u001b[39;00m\n\u001b[0;32m     80\u001b[0m dC_dB2 \u001b[38;5;241m=\u001b[39m dC_dA2 \u001b[38;5;241m@\u001b[39m dA2_dZ2 \u001b[38;5;241m*\u001b[39m dZ2_dB2         \u001b[38;5;66;03m# (1x4) @ (4x1) * (1x1) = (1x1)   == (4x1)  B2\u001b[39;00m\n\u001b[0;32m     81\u001b[0m dC_dA1 \u001b[38;5;241m=\u001b[39m dC_dA2\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m dA2_dZ2\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m dZ2_dA1     \u001b[38;5;66;03m# (4x1) @ (1x4) @ (4x8) = (4x8)   == (8x1)  A1\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 4)"
     ]
    }
   ],
   "source": [
    "from tkinter import X, Y\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "all_data = pd.read_csv(\"pasajeros_actualizado.csv\")\n",
    "\n",
    "# Extraer columnas de entrada, escalarlas dividiendo por 255\n",
    "all_inputs = all_data.iloc[:, 0:16].values\n",
    "all_outputs = all_data.iloc[:, -1].values\n",
    "scaler = StandardScaler()\n",
    "all_inputs = scaler.fit_transform(all_inputs)\n",
    "\n",
    "# Dividir los conjuntos de datos de entrenamiento y prueba\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(all_inputs, all_outputs, test_size=1 / 3)\n",
    "n = X_train.shape[0]\n",
    "print(n)\n",
    "\n",
    "# Funciones de activación\n",
    "relu = lambda x: np.maximum(x, 0)\n",
    "logistic = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Construir red neuronal con pesos y sesgos inicializados aleatoriamente\n",
    "np.random.seed(79)\n",
    "w_hidden = np.random.rand(8, 16)        # (8x16)\n",
    "w_hidden2 = np.random.rand(4, 8)        # (4x8)\n",
    "w_output = np.random.rand(1, 4)         # (1x4)\n",
    "\n",
    "b_hidden = np.random.rand(8, 1)         # (8x1)\n",
    "b_hidden2 = np.random.rand(4, 1)        # (4x1)\n",
    "b_output = np.random.rand(1, 1)         # (1x1)\n",
    "\n",
    "# Ejecutar entradas a través de la red neuronal para obtener salidas predichas\n",
    "def forward_prop(X):\n",
    "    Z1 = w_hidden @ X + b_hidden        # (8x16) @ (16x1) + (8x1) = (8x1)\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = w_hidden2 @ A1 + b_hidden2     # (4x8) @ (8x1) + (4x1) = (4x1)\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = w_output @ A2 + b_output       # (1x4) @ (4x1) + (1x1) = (1x1)\n",
    "    A3 = logistic(Z3)\n",
    "    return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "# Calculo de precisión\n",
    "test_predictions = forward_prop(X_test.transpose())[4]  # me interesa solo la capa de salida, A2\n",
    "test_comparisons = np.equal((test_predictions >= .5).flatten().astype(int), Y_test)\n",
    "accuracy = sum(test_comparisons.astype(int) / X_test.shape[0])\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "# Tasa de aprendizaje\n",
    "L = 0.01\n",
    "\n",
    "# Derivadas de las funciones de activación\n",
    "d_relu = lambda x: x > 0\n",
    "d_logistic = lambda x: np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "\n",
    "# Función de backward propagation\n",
    "def backward_prop(Z1, A1, Z2, A2, Z3, A3, X, Y):\n",
    "    dC_dA3 =   2 * A3 - 2 * Y       # (1x1)  == (1x1) A3\n",
    "    dA3_dZ3 =  d_logistic(Z3)       # (1x1)\n",
    "    dZ3_dA2 =  w_output             # (1x4)\n",
    "    dZ3_dW3 =  A2                   # (4x1)\n",
    "    dZ3_dB3 =  1                    # (1x1)\n",
    "    \n",
    "    dA2_dZ2 =  d_relu(Z2)           # (4x1)\n",
    "    dZ2_dA1 =  w_hidden2            # (4x8)\n",
    "    dZ2_dW2 =  A1                   # (8x1)\n",
    "    dZ2_dB2 =  1                    # (1x1)\n",
    "    \n",
    "    dA1_dZ1 =  d_relu(Z1)           # (8x1)\n",
    "    dZ1_dW1 =  X                    # (16x1)\n",
    "    dZ1_dB1 =  1                    # (1x1)\n",
    "    \n",
    "    dC_dW3 = dC_dA3 @ dA3_dZ3 @ dZ3_dW3.T       # (1x1) @ (1x1) @ (1x4) = (1x4)   == (1x4)  W3 \n",
    "    dC_dB3 = dC_dA3 @ dA3_dZ3 * dZ3_dB3         # (1x1) @ (1x1) * (1x1) = (1x1)   == (1x1)  B3\n",
    "    dC_dA2 = (dC_dA3 @ dA3_dZ3 @ dZ3_dA2).T     # (1x1) @ (1x1) @ (1x4) = (4x1)   == (4x1)  A2\n",
    "    \n",
    "    dC_dW2 = dC_dA2.T @ (dA2_dZ2 * dZ2_dW2.T)   # (1x4) @ ((4x1) * (1x8)) = (1x8) == (4x8)  W2\n",
    "    dC_dB2 = dC_dA2 @ dA2_dZ2 * dZ2_dB2         # (1x4) @ (4x1) * (1x1) = (1x1)   == (4x1)  B2\n",
    "    dC_dA1 = dC_dA2.T @ dA2_dZ2.T @ dZ2_dA1     # (4x1) @ (1x4) @ (4x8) = (4x8)   == (8x1)  A1\n",
    "    \n",
    "    dC_dW1 = dC_dA1 @ dA1_dZ1 @ dZ1_dW1.T       # (4x8) @ (8X1) @ (1X16) = (4x16) == (8x16) W1\n",
    "    dC_dB1 = dA1_dZ1.T @ dC_dA1.T * dZ1_dB1     # (1x8) @ (8x4) * (1x1) = (1x4)   == (8x1)  B1\n",
    "    \n",
    "    return dC_dW1, dC_dB1, dC_dW2, dC_dB2, dC_dW3, dC_dB3\n",
    "\n",
    "\n",
    "for i in range(150_000):\n",
    "    # seleccionar aleatoriamente uno de los datos de entrenamiento\n",
    "    idx = np.random.choice(n, 1, replace=False)\n",
    "    X_sample = X_train[idx].transpose()\n",
    "    Y_sample = Y_train[idx]\n",
    "\n",
    "    # pasar datos seleccionados aleatoriamente a través de la red neuronal\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward_prop(X_sample)\n",
    "\n",
    "    # distribuir error a través de la retropropagación\n",
    "    # y devolver pendientes para pesos y sesgos\n",
    "    dW1, dB1, dW2, dB2, dW3, dB3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, X_sample, Y_sample)\n",
    "\n",
    "    # actualizar pesos y sesgos\n",
    "    w_hidden -= L * dW1\n",
    "    b_hidden -= L * dB1\n",
    "    w_hidden2 -= L * dW2\n",
    "    b_hidden2 -= L * dB2\n",
    "    w_output -= L * dW3\n",
    "    b_output -= L * dB3\n",
    "    \n",
    "print(\"Shapes:\")\n",
    "print(\"w_hidden:\", w_hidden.shape)\n",
    "print(\"dW1:\", dW1.shape)\n",
    "print(\"w_hidden2:\", w_hidden2.shape)\n",
    "print(\"dW2:\", dW2.shape)\n",
    "print(\"w_output:\", w_output.shape)\n",
    "print(\"dW1:\", dW3.shape)\n",
    "\n",
    "# Calculo de precisión\n",
    "test_predictions = forward_prop(X_test.transpose())[4]  # me interesa solo la capa de salida, A2\n",
    "test_comparisons = np.equal((test_predictions >= .5).flatten().astype(int), Y_test)\n",
    "accuracy = sum(test_comparisons.astype(int) / X_test.shape[0])\n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sympy import *\\n\\nW1, W2, W3, B1, B2, B3, A1, A2, A3, Z1, Z2, Z3, X, Y =     symbols(\\'W1 W2 W3 B1 B2 B3 A1 A2 A3 Z1 Z2 Z3 X Y\\')\\n\\n# Derivada de la función costo respecto A3\\nC = (A3 - Y)**2\\ndC_dA3 = diff(C, A3)\\nprint(\"dC_dA3 = \", dC_dA3) # 2*A3 - 2*Y\\n\\n# Derivada de A3 respecto de Z3\\nlogistic = lambda x: 1 / (1 + exp(-x))\\n_A3 = logistic(Z3)\\ndA3_dZ3 = diff(_A3, Z3)\\nprint(\"dA3_dZ3 = \", dA3_dZ3) # exp(-Z3)/(1 + exp(-Z3))**2\\n\\n# Derivada de Z3 respecto a A2\\n_Z3 = A2*W3 + B3\\ndZ3_dA2 = diff(_Z3, A2)\\nprint(\"dZ3_dA2 = \", dZ3_dA2) # W3\\n\\n# Derivada de Z3 respecto a W3\\ndZ3_dW3 = diff(_Z3, W3)\\nprint(\"dZ3_dW3 = \", dZ3_dW3) # A2\\n\\n# Derivada de Z3 respecto a B3\\ndZ3_dB3 = diff(_Z3, B3)\\nprint(\"dZ3_dB3 = \", dZ3_dB3) # 1\\n\\n# Derivada de A2 respecto de Z2\\n_A2 = logistic(Z2)\\ndA2_dZ2 = diff(_A2, Z2)\\nprint(\"dA2_dZ2 = \", dA2_dZ2) # exp(-Z2)/(1 + exp(-Z2))**2\\n\\n# Derivada de Z2 respecto a A1\\n_Z2 = A1*W2 + B2\\ndZ2_dA1 = diff(_Z2, A1)\\nprint(\"dZ2_dA1 = \", dZ2_dA1) # W2\\n\\n# Derivada de Z2 respecto a W2\\ndZ2_dW2 = diff(_Z2, W2)\\nprint(\"dZ2_dW2 = \", dZ2_dW2) # A1\\n\\n# Derivada de Z2 respecto a B2\\ndZ2_dB2 = diff(_Z2, B2)\\nprint(\"dZ2_dB2 = \", dZ2_dB2) # 1\\n\\n# Derivada de A1 respecto de Z1\\nrelu = lambda x: Max(x, 0)\\n_A1 = relu(Z1)\\n\\nd_relu = lambda x: x > 0 # Pendiente es 1 para los positivos, 0 para los negativos\\ndA1_dZ1 = d_relu(Z1)\\nprint(\"dA1_dZ1 = \", dA1_dZ1) # Z1 > 0\\n\\n# Derivada de Z1 respecto a W1\\n_Z1 = X*W1 + B1\\ndZ1_dW1 = diff(_Z1, W1)\\nprint(\"dZ1_dW1 = \", dZ1_dW1) # X\\n\\n# Derivada de Z1 respecto a B1\\ndZ1_dB1 = diff(_Z1, B1)\\nprint(\"dZ1_dB1 = \", dZ1_dB1) # 1\\n'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sympy import *\n",
    "\n",
    "W1, W2, W3, B1, B2, B3, A1, A2, A3, Z1, Z2, Z3, X, Y = \\\n",
    "    symbols('W1 W2 W3 B1 B2 B3 A1 A2 A3 Z1 Z2 Z3 X Y')\n",
    "\n",
    "# Derivada de la función costo respecto A3\n",
    "C = (A3 - Y)**2\n",
    "dC_dA3 = diff(C, A3)\n",
    "print(\"dC_dA3 = \", dC_dA3) # 2*A3 - 2*Y\n",
    "\n",
    "# Derivada de A3 respecto de Z3\n",
    "logistic = lambda x: 1 / (1 + exp(-x))\n",
    "_A3 = logistic(Z3)\n",
    "dA3_dZ3 = diff(_A3, Z3)\n",
    "print(\"dA3_dZ3 = \", dA3_dZ3) # exp(-Z3)/(1 + exp(-Z3))**2\n",
    "\n",
    "# Derivada de Z3 respecto a A2\n",
    "_Z3 = A2*W3 + B3\n",
    "dZ3_dA2 = diff(_Z3, A2)\n",
    "print(\"dZ3_dA2 = \", dZ3_dA2) # W3\n",
    "\n",
    "# Derivada de Z3 respecto a W3\n",
    "dZ3_dW3 = diff(_Z3, W3)\n",
    "print(\"dZ3_dW3 = \", dZ3_dW3) # A2\n",
    "\n",
    "# Derivada de Z3 respecto a B3\n",
    "dZ3_dB3 = diff(_Z3, B3)\n",
    "print(\"dZ3_dB3 = \", dZ3_dB3) # 1\n",
    "\n",
    "# Derivada de A2 respecto de Z2\n",
    "_A2 = logistic(Z2)\n",
    "dA2_dZ2 = diff(_A2, Z2)\n",
    "print(\"dA2_dZ2 = \", dA2_dZ2) # exp(-Z2)/(1 + exp(-Z2))**2\n",
    "\n",
    "# Derivada de Z2 respecto a A1\n",
    "_Z2 = A1*W2 + B2\n",
    "dZ2_dA1 = diff(_Z2, A1)\n",
    "print(\"dZ2_dA1 = \", dZ2_dA1) # W2\n",
    "\n",
    "# Derivada de Z2 respecto a W2\n",
    "dZ2_dW2 = diff(_Z2, W2)\n",
    "print(\"dZ2_dW2 = \", dZ2_dW2) # A1\n",
    "\n",
    "# Derivada de Z2 respecto a B2\n",
    "dZ2_dB2 = diff(_Z2, B2)\n",
    "print(\"dZ2_dB2 = \", dZ2_dB2) # 1\n",
    "\n",
    "# Derivada de A1 respecto de Z1\n",
    "relu = lambda x: Max(x, 0)\n",
    "_A1 = relu(Z1)\n",
    "\n",
    "d_relu = lambda x: x > 0 # Pendiente es 1 para los positivos, 0 para los negativos\n",
    "dA1_dZ1 = d_relu(Z1)\n",
    "print(\"dA1_dZ1 = \", dA1_dZ1) # Z1 > 0\n",
    "\n",
    "# Derivada de Z1 respecto a W1\n",
    "_Z1 = X*W1 + B1\n",
    "dZ1_dW1 = diff(_Z1, W1)\n",
    "print(\"dZ1_dW1 = \", dZ1_dW1) # X\n",
    "\n",
    "# Derivada de Z1 respecto a B1\n",
    "dZ1_dB1 = diff(_Z1, B1)\n",
    "print(\"dZ1_dB1 = \", dZ1_dB1) # 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[255], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m Z1, A1, Z2, A2, Z3, A3 \u001b[38;5;241m=\u001b[39m forward_prop(X_sample)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# distribuir error a través de la retropropagación\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# y devolver pendientes para pesos y sesgos\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m dW1, dB1, dW2, dB2, dW3, dB3 \u001b[38;5;241m=\u001b[39m \u001b[43mbackward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# actualizar pesos y sesgos\u001b[39;00m\n\u001b[0;32m     54\u001b[0m w_hidden \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m L \u001b[38;5;241m*\u001b[39m dW1\u001b[38;5;241m.\u001b[39mT\n",
      "Cell \u001b[1;32mIn[255], line 29\u001b[0m, in \u001b[0;36mbackward_prop\u001b[1;34m(Z1, A1, Z2, A2, Z3, A3, X, Y)\u001b[0m\n\u001b[0;32m     26\u001b[0m dC_dB3 \u001b[38;5;241m=\u001b[39m dC_dA3 \u001b[38;5;241m@\u001b[39m dA3_dZ3 \u001b[38;5;241m*\u001b[39m dZ3_dB3                 \u001b[38;5;66;03m# (1x1) @ (1x1) * (1x1) = (1x1)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m dC_dA2 \u001b[38;5;241m=\u001b[39m dC_dA3 \u001b[38;5;241m@\u001b[39m dA3_dZ3 \u001b[38;5;241m@\u001b[39m dZ3_dA2                 \u001b[38;5;66;03m# (1x1) @ (1x1) @ (1x4) = (1x4)\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m dC_dW2 \u001b[38;5;241m=\u001b[39m \u001b[43mdZ2_dW2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdA2_dZ2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdC_dA2\u001b[49m\u001b[43m)\u001b[49m               \u001b[38;5;66;03m# (8x1) @ (1x4) = (8x4)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m dC_dB2 \u001b[38;5;241m=\u001b[39m dC_dA2 \u001b[38;5;241m@\u001b[39m dA2_dZ2\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m*\u001b[39m dZ2_dB2               \u001b[38;5;66;03m# (1x1) @ (1x4) * (1x1) = (1x4)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m dC_dA1 \u001b[38;5;241m=\u001b[39m dZ2_dA1\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m (dA2_dZ2 \u001b[38;5;241m*\u001b[39m dC_dA2)             \u001b[38;5;66;03m# (8x4) @ ((4x1) * (1x4)) = (8x4) @ (4x1) = (8x1)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 1)"
     ]
    }
   ],
   "source": [
    "'''# Tasa de aprendizaje\n",
    "L = 0.01\n",
    "\n",
    "# Derivadas de las funciones de activación\n",
    "d_relu = lambda x: x > 0\n",
    "d_logistic = lambda x: np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "\n",
    "# Función de backward propagation\n",
    "def backward_prop(Z1, A1, Z2, A2, Z3, A3, X, Y):\n",
    "    dC_dA3 =   2 * A3 - 2 * Y       # (1x1)\n",
    "    dA3_dZ3 =  d_logistic(Z3)       # (1x1)\n",
    "    dZ3_dA2 =  w_output             # (1x4)\n",
    "    dZ3_dW3 =  A2                   # (4x1)\n",
    "    dZ3_dB3 =  1                    # (1x1)\n",
    "    \n",
    "    dA2_dZ2 =  d_relu(Z2)           # (4x1)\n",
    "    dZ2_dA1 =  w_hidden2            # (4x8)\n",
    "    dZ2_dW2 =  A1                   # (8x1)\n",
    "    dZ2_dB2 =  1                    # (1x1)\n",
    "    \n",
    "    dA1_dZ1 =  d_relu(Z1)           # (8x1)\n",
    "    dZ1_dW1 =  X                    # (16x1)\n",
    "    dZ1_dB1 =  1                    # (1x1)\n",
    "    \n",
    "    dC_dW3 = dC_dA3 @ dA3_dZ3 @ dZ3_dW3.T               # (1x1) @ (1x1) @ (1x4) = (1x4)\n",
    "    dC_dB3 = dC_dA3 @ dA3_dZ3 * dZ3_dB3                 # (1x1) @ (1x1) * (1x1) = (1x1)\n",
    "    dC_dA2 = dC_dA3 @ dA3_dZ3 @ dZ3_dA2                 # (1x1) @ (1x1) @ (1x4) = (1x4)\n",
    "    \n",
    "    dC_dW2 = dZ2_dW2 @ (dA2_dZ2 * dC_dA2)               # (8x1) @ (1x4) = (8x4)\n",
    "    dC_dB2 = dC_dA2 @ dA2_dZ2.T * dZ2_dB2               # (1x1) @ (1x4) * (1x1) = (1x4)\n",
    "    dC_dA1 = dZ2_dA1.T @ (dA2_dZ2 * dC_dA2)             # (8x4) @ ((4x1) * (1x4)) = (8x4) @ (4x1) = (8x1)\n",
    "    \n",
    "    dC_dW1 = (dZ1_dW1 @ (dA1_dZ1 * dC_dA1).T)           # (16x1) @ ((8x1) * (8x1)).T = ((16x1) @ (1x8)) = (8x16)\n",
    "    dC_dB1 = dZ1_dB1 * (dA1_dZ1.T * dC_dA1)             # (1x1) * ((1x8) * (8x1)) = (1x1) * (8x1)\n",
    "    \n",
    "\n",
    "    return dC_dW1, dC_dB1, dC_dW2, dC_dB2, dC_dW3, dC_dB3\n",
    "\n",
    "\n",
    "for i in range(150_000):\n",
    "    # seleccionar aleatoriamente uno de los datos de entrenamiento\n",
    "    idx = np.random.choice(n, 1, replace=False)\n",
    "    X_sample = X_train[idx].transpose()\n",
    "    Y_sample = Y_train[idx].reshape(1, -1)\n",
    "\n",
    "    # pasar datos seleccionados aleatoriamente a través de la red neuronal\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward_prop(X_sample)\n",
    "\n",
    "    # distribuir error a través de la retropropagación\n",
    "    # y devolver pendientes para pesos y sesgos\n",
    "    dW1, dB1, dW2, dB2, dW3, dB3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, X_sample, Y_sample)\n",
    "    \n",
    "    # actualizar pesos y sesgos\n",
    "    w_hidden -= L * dW1.T\n",
    "    b_hidden -= L * dB1\n",
    "    w_hidden2 -= L * dW2\n",
    "    b_hidden2 -= L * dB2\n",
    "    w_output -= L * dW3\n",
    "    b_output -= L * dB3\n",
    "    \n",
    "print(\"Shapes:\")\n",
    "print(\"w_hidden:\", w_hidden.shape)\n",
    "print(\"dW1:\", dW1.shape)\n",
    "print(\"w_hidden2:\", w_hidden2.shape)\n",
    "print(\"dW2:\", dW2.shape)\n",
    "print(\"w_output:\", w_output.shape)\n",
    "print(\"dW1:\", dW3.shape)\n",
    "\n",
    "# Calculo de precisión\n",
    "test_predictions = forward_prop(X_test.transpose())[4]  # me interesa solo la capa de salida, A2\n",
    "test_comparisons = np.equal((test_predictions >= .5).flatten().astype(int), Y_test)\n",
    "accuracy = sum(test_comparisons.astype(int) / X_test.shape[0])\n",
    "\n",
    "print(accuracy)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
